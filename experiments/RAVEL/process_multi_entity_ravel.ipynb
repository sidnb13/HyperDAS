{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import datasets\n",
    "import torch\n",
    "from transformers import AutoTokenizer, LlamaForCausalLM\n",
    "\n",
    "from src.hyperdas.data_utils import (\n",
    "    filter_dataset,\n",
    "    generate_ravel_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Field', 'Award Year', 'Birth Year', 'Country of Birth', 'Gender'])\n"
     ]
    }
   ],
   "source": [
    "p = \"/workspace/HyperDAS/assets/data/ravel/ravel_nobel_prize_winner_attribute_to_prompts.json\"\n",
    "with open(p, \"r\") as f:\n",
    "    prompts = json.load(f)\n",
    "    print(prompts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Duty', 'Gender Bias', 'Industry', 'Work Location'])\n"
     ]
    }
   ],
   "source": [
    "p = \"/workspace/HyperDAS/assets/data/ravel/ravel_occupation_attribute_to_prompts.json\"\n",
    "with open(p, \"r\") as f:\n",
    "    prompts = json.load(f)\n",
    "    print(prompts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"meta-llama/Meta-Llama-3-8B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39c97bb694ee48b49a5cb44cfbbfb253",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_name_or_path, torch_dtype=torch.bfloat16, device_map={\"\": 0}\n",
    ")\n",
    "\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [00:42, 14.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6333; filtered out 3667 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "396it [00:26, 15.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9958945207642508; filtered out 26 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "395it [00:26, 15.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9995243380371016; filtered out 3 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [00:41, 15.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6186; filtered out 3814 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "387it [00:25, 15.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9964435822825736; filtered out 22 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "386it [00:25, 15.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9990266060999351; filtered out 6 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [00:43, 14.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7362; filtered out 2638 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "461it [00:32, 14.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9980983428416191; filtered out 14 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "460it [00:32, 14.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9986390854654328; filtered out 10 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [00:43, 14.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.715; filtered out 2850 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "447it [00:31, 14.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9952447552447552; filtered out 34 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "445it [00:31, 14.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9977515458122541; filtered out 16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [00:45, 13.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2814; filtered out 7186 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "176it [00:12, 13.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9832977967306326; filtered out 47 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "173it [00:12, 13.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9916877484640405; filtered out 23 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [00:45, 13.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.249; filtered out 7510 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "156it [00:11, 13.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9863453815261044; filtered out 34 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "154it [00:11, 13.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.995114006514658; filtered out 12 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# CHANGE: {base_entity} → {source_entity} | ATTR: {target_attribute}\n",
    "from collections import defaultdict\n",
    "\n",
    "all_attributes = {\n",
    "    \"city\": [\"Country\", \"Continent\", \"Language\", \"Latitude\", \"Longitude\", \"Timezone\"],\n",
    "    \"nobel_prize_winner\": [\n",
    "        \"Field\",\n",
    "        \"Award Year\",\n",
    "        \"Birth Year\",\n",
    "        \"Country of Birth\",\n",
    "        \"Gender\",\n",
    "    ],\n",
    "    \"occupation\": [\"Duty\", \"Industry\", \"Work Location\"],\n",
    "}\n",
    "\n",
    "target_attributes = {\n",
    "    \"city\": [\"Country\"],\n",
    "    \"nobel_prize_winner\": [\"Field\"],\n",
    "    \"occupation\": [\"Duty\"],\n",
    "}\n",
    "\n",
    "\n",
    "domains = [\"city\", \"nobel_prize_winner\", \"occupation\"]\n",
    "all_datasets = defaultdict(list)\n",
    "\n",
    "for domain in domains:\n",
    "    for split in [\"train\", \"test\"]:\n",
    "        args = {\n",
    "            \"n_samples\": 10000,\n",
    "            \"root_path\": \"/workspace/HyperDAS/assets/data/ravel\",\n",
    "            \"target_attributes\": target_attributes[domain],\n",
    "            \"isolate_attributes\": list(\n",
    "                set(all_attributes[domain]) - set(target_attributes[domain])\n",
    "            ),\n",
    "            \"template_split\": split,\n",
    "            \"entity_split\": split,\n",
    "            \"domain\": domain,\n",
    "            # \"edit_instruction_template\": \"CHANGE: {base_entity} -> {source_entity} | ATTR: {random_target_attribute}\",\n",
    "        }\n",
    "\n",
    "        dataset = generate_ravel_dataset(**args)\n",
    "\n",
    "        dataset = filter_dataset(model, tokenizer, dataset, batch_size=16)\n",
    "        dataset = filter_dataset(model, tokenizer, dataset, batch_size=16)\n",
    "        dataset = filter_dataset(model, tokenizer, dataset, batch_size=16)\n",
    "\n",
    "        metadata = {\n",
    "            **args,\n",
    "            \"target_attributes\": tuple(args[\"target_attributes\"]),\n",
    "            \"isolate_attributes\": tuple(args[\"isolate_attributes\"]),\n",
    "        }\n",
    "\n",
    "        all_datasets[split].append((dataset, metadata))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8bb06a0e7534989a5f78ac6ce7b08b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/16386 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67f34b1eacdb4d39ae22533e25ab4180",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/15702 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for split, dataset_list in all_datasets.items():\n",
    "    dataset_list, metadata_list = zip(*dataset_list)\n",
    "    combined = datasets.concatenate_datasets(dataset_list)\n",
    "    path = f\"/workspace/HyperDAS/experiments/RAVEL/data/city_nobel_prize_winner_occupation_{split}\"\n",
    "    combined.save_to_disk(path)\n",
    "    with open(os.path.join(path, \"metadata.json\"), \"w\") as f:\n",
    "        json.dump({\"metadata\": metadata_list}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE: {base_entity} → {source_entity} | ATTR: {target_attribute}\n",
    "from collections import defaultdict\n",
    "\n",
    "all_attributes = {\n",
    "    \"city\": [\"Country\", \"Continent\", \"Language\", \"Latitude\", \"Longitude\", \"Timezone\"],\n",
    "    \"nobel_prize_winner\": [\n",
    "        \"Field\",\n",
    "        \"Award Year\",\n",
    "        \"Birth Year\",\n",
    "        \"Country of Birth\",\n",
    "        \"Gender\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "target_attributes = {\n",
    "    \"city\": [\"Country\"],\n",
    "    \"nobel_prize_winner\": [\"Field\"],\n",
    "}\n",
    "\n",
    "\n",
    "domains = [\"city\", \"nobel_prize_winner\"]\n",
    "all_datasets = defaultdict(list)\n",
    "\n",
    "for domain in domains:\n",
    "    for split in [\"train\", \"test\"]:\n",
    "        args = {\n",
    "            \"n_samples\": 10000,\n",
    "            \"root_path\": \"/workspace/HyperDAS/assets/data/ravel\",\n",
    "            \"target_attributes\": target_attributes[domain],\n",
    "            \"isolate_attributes\": list(\n",
    "                set(all_attributes[domain]) - set(target_attributes[domain])\n",
    "            ),\n",
    "            \"template_split\": split,\n",
    "            \"entity_split\": split,\n",
    "            \"domain\": domain,\n",
    "            # \"edit_instruction_template\": \"CHANGE: {base_entity} -> {source_entity} | ATTR: {random_target_attribute}\",\n",
    "        }\n",
    "\n",
    "        dataset = generate_ravel_dataset(**args)\n",
    "\n",
    "        dataset = filter_dataset(model, tokenizer, dataset, batch_size=16)\n",
    "        dataset = filter_dataset(model, tokenizer, dataset, batch_size=16)\n",
    "        dataset = filter_dataset(model, tokenizer, dataset, batch_size=16)\n",
    "\n",
    "        metadata = {\n",
    "            **args,\n",
    "            \"target_attributes\": tuple(args[\"target_attributes\"]),\n",
    "            \"isolate_attributes\": tuple(args[\"isolate_attributes\"]),\n",
    "        }\n",
    "\n",
    "        all_datasets[split].append((dataset, metadata))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split, dataset_list in all_datasets.items():\n",
    "    dataset_list, metadata_list = zip(*dataset_list)\n",
    "    combined = datasets.concatenate_datasets(dataset_list)\n",
    "    path = f\"/workspace/HyperDAS/experiments/RAVEL/data/city_nobel_prize_winner_{split}\"\n",
    "    combined.save_to_disk(path)\n",
    "    with open(os.path.join(path, \"metadata.json\"), \"w\") as f:\n",
    "        json.dump({\"metadata\": metadata_list}, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
