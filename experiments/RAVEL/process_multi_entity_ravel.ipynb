{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import datasets\n",
    "import torch\n",
    "from transformers import AutoTokenizer, LlamaForCausalLM\n",
    "\n",
    "from src.hyperdas.data_utils import (\n",
    "    filter_dataset,\n",
    "    generate_ravel_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Field', 'Award Year', 'Birth Year', 'Country of Birth', 'Gender'])\n"
     ]
    }
   ],
   "source": [
    "p = \"/workspace/HyperDAS/assets/data/ravel/ravel_nobel_prize_winner_attribute_to_prompts.json\"\n",
    "with open(p, \"r\") as f:\n",
    "    prompts = json.load(f)\n",
    "    print(prompts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Duty', 'Gender Bias', 'Industry', 'Work Location'])\n"
     ]
    }
   ],
   "source": [
    "p = \"/workspace/HyperDAS/assets/data/ravel/ravel_occupation_attribute_to_prompts.json\"\n",
    "with open(p, \"r\") as f:\n",
    "    prompts = json.load(f)\n",
    "    print(prompts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"meta-llama/Meta-Llama-3-8B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e345f7335f6340ffabbc5af969375465",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_name_or_path, torch_dtype=torch.bfloat16, device_map={\"\": 0}\n",
    ")\n",
    "\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [00:48, 13.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6289; filtered out 3711 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "394it [00:29, 13.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9984099220861822; filtered out 10 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "393it [00:29, 13.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9982481286829112; filtered out 11 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [00:47, 13.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6137; filtered out 3863 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "384it [00:28, 13.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9967410787029494; filtered out 20 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "383it [00:28, 13.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.998855648193559; filtered out 7 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [00:49, 12.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7337; filtered out 2663 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "459it [00:37, 12.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9970014992503748; filtered out 22 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "458it [00:37, 12.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9986329460013671; filtered out 10 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [00:49, 12.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7145; filtered out 2855 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "447it [00:36, 12.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9960811756473058; filtered out 28 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "445it [00:36, 12.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9978923703807784; filtered out 15 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [00:52, 11.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2814; filtered out 7186 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "176it [00:14, 11.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9832977967306326; filtered out 47 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "173it [00:14, 11.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9916877484640405; filtered out 23 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [00:52, 11.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.249; filtered out 7510 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "156it [00:13, 11.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9863453815261044; filtered out 34 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "154it [00:13, 11.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.995114006514658; filtered out 12 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# CHANGE: {base_entity} → {source_entity} | ATTR: {target_attribute}\n",
    "from collections import defaultdict\n",
    "\n",
    "all_attributes = {\n",
    "    \"city\": [\"Country\", \"Continent\", \"Language\", \"Latitude\", \"Longitude\", \"Timezone\"],\n",
    "    \"nobel_prize_winner\": [\n",
    "        \"Field\",\n",
    "        \"Award Year\",\n",
    "        \"Birth Year\",\n",
    "        \"Country of Birth\",\n",
    "        \"Gender\",\n",
    "    ],\n",
    "    \"occupation\": [\"Duty\", \"Industry\", \"Work Location\"],\n",
    "}\n",
    "\n",
    "target_attributes = {\n",
    "    \"city\": [\"Country\"],\n",
    "    \"nobel_prize_winner\": [\"Field\"],\n",
    "    \"occupation\": [\"Duty\"],\n",
    "}\n",
    "\n",
    "\n",
    "domains = [\"city\", \"nobel_prize_winner\", \"occupation\"]\n",
    "all_datasets = defaultdict(list)\n",
    "\n",
    "for domain in domains:\n",
    "    for split in [\"train\", \"test\"]:\n",
    "        args = {\n",
    "            \"n_samples\": 10000,\n",
    "            \"root_path\": \"/workspace/HyperDAS/assets/data/ravel\",\n",
    "            \"target_attributes\": target_attributes[domain],\n",
    "            \"isolate_attributes\": list(\n",
    "                set(all_attributes[domain]) - set(target_attributes[domain])\n",
    "            ),\n",
    "            \"template_split\": split,\n",
    "            \"entity_split\": split,\n",
    "            \"domain\": domain,\n",
    "            # \"edit_instruction_template\": \"CHANGE: {base_entity} -> {source_entity} | ATTR: {random_target_attribute}\",\n",
    "        }\n",
    "\n",
    "        dataset = generate_ravel_dataset(**args)\n",
    "\n",
    "        dataset = filter_dataset(model, tokenizer, dataset, batch_size=16)\n",
    "        dataset = filter_dataset(model, tokenizer, dataset, batch_size=16)\n",
    "        dataset = filter_dataset(model, tokenizer, dataset, batch_size=16)\n",
    "\n",
    "        metadata = {\n",
    "            **args,\n",
    "            \"target_attributes\": tuple(args[\"target_attributes\"]),\n",
    "            \"isolate_attributes\": tuple(args[\"isolate_attributes\"]),\n",
    "        }\n",
    "\n",
    "        all_datasets[split].append((dataset, metadata))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82ac71206cb04023baa71e5a49f3aba7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/16317 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "069c01360eb545d1bdbb01d30b7f1995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/15656 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for split, dataset_list in all_datasets.items():\n",
    "    dataset_list, metadata_list = zip(*dataset_list)\n",
    "    combined = datasets.concatenate_datasets(dataset_list)\n",
    "    path = f\"/workspace/HyperDAS/experiments/RAVEL/data/city_nobel_prize_winner_occupation_{split}\"\n",
    "    combined.save_to_disk(path)\n",
    "    with open(os.path.join(path, \"metadata.json\"), \"w\") as f:\n",
    "        json.dump({\"metadata\": metadata_list}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [00:47, 13.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6289; filtered out 3711 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "394it [00:29, 13.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9984099220861822; filtered out 10 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "393it [00:29, 13.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9982481286829112; filtered out 11 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [00:47, 13.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6137; filtered out 3863 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "384it [00:28, 13.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9967410787029494; filtered out 20 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "383it [00:28, 13.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.998855648193559; filtered out 7 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [00:49, 12.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7337; filtered out 2663 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "459it [00:37, 12.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9970014992503748; filtered out 22 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "458it [00:37, 12.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9986329460013671; filtered out 10 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [00:49, 12.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7145; filtered out 2855 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "447it [00:36, 12.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9960811756473058; filtered out 28 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "445it [00:36, 12.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9978923703807784; filtered out 15 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# CHANGE: {base_entity} → {source_entity} | ATTR: {target_attribute}\n",
    "from collections import defaultdict\n",
    "\n",
    "all_attributes = {\n",
    "    \"city\": [\"Country\", \"Continent\", \"Language\", \"Latitude\", \"Longitude\", \"Timezone\"],\n",
    "    \"nobel_prize_winner\": [\n",
    "        \"Field\",\n",
    "        \"Award Year\",\n",
    "        \"Birth Year\",\n",
    "        \"Country of Birth\",\n",
    "        \"Gender\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "target_attributes = {\n",
    "    \"city\": [\"Country\"],\n",
    "    \"nobel_prize_winner\": [\"Field\"],\n",
    "}\n",
    "\n",
    "\n",
    "domains = [\"city\", \"nobel_prize_winner\"]\n",
    "all_datasets = defaultdict(list)\n",
    "\n",
    "for domain in domains:\n",
    "    for split in [\"train\", \"test\"]:\n",
    "        args = {\n",
    "            \"n_samples\": 10000,\n",
    "            \"root_path\": \"/workspace/HyperDAS/assets/data/ravel\",\n",
    "            \"target_attributes\": target_attributes[domain],\n",
    "            \"isolate_attributes\": list(\n",
    "                set(all_attributes[domain]) - set(target_attributes[domain])\n",
    "            ),\n",
    "            \"template_split\": split,\n",
    "            \"entity_split\": split,\n",
    "            \"domain\": domain,\n",
    "            # \"edit_instruction_template\": \"CHANGE: {base_entity} -> {source_entity} | ATTR: {random_target_attribute}\",\n",
    "        }\n",
    "\n",
    "        dataset = generate_ravel_dataset(**args)\n",
    "\n",
    "        dataset = filter_dataset(model, tokenizer, dataset, batch_size=16)\n",
    "        dataset = filter_dataset(model, tokenizer, dataset, batch_size=16)\n",
    "        dataset = filter_dataset(model, tokenizer, dataset, batch_size=16)\n",
    "\n",
    "        metadata = {\n",
    "            **args,\n",
    "            \"target_attributes\": tuple(args[\"target_attributes\"]),\n",
    "            \"isolate_attributes\": tuple(args[\"isolate_attributes\"]),\n",
    "        }\n",
    "\n",
    "        all_datasets[split].append((dataset, metadata))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7b4f5a2186946c4912fe86571131a21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/13573 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32b4ebc81c814a6ea1bdee64a55f84cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/13212 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for split, dataset_list in all_datasets.items():\n",
    "    dataset_list, metadata_list = zip(*dataset_list)\n",
    "    combined = datasets.concatenate_datasets(dataset_list)\n",
    "    path = f\"/workspace/HyperDAS/experiments/RAVEL/data/city_nobel_prize_winner_{split}\"\n",
    "    combined.save_to_disk(path)\n",
    "    with open(os.path.join(path, \"metadata.json\"), \"w\") as f:\n",
    "        json.dump({\"metadata\": metadata_list}, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
